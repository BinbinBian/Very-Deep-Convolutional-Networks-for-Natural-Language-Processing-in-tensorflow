{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "samples = {}\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\"\n",
    "FEATURE_LEN =1014\n",
    "cdict = {}\n",
    "for i,c in enumerate(ALPHABET):\n",
    "    cdict[c] = i + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = {}\n",
    "with open('dbpedia_data/train.csv') as f:\n",
    "    reader = csv.DictReader(f,fieldnames=['class'],restkey='fields')\n",
    "    for row in reader:\n",
    "        label = row['class']\n",
    "        if label not in samples:\n",
    "            samples[label] = []\n",
    "        sample = np.ones(FEATURE_LEN)\n",
    "        count = 0\n",
    "        for field in row['fields']:\n",
    "            for char in field.lower():\n",
    "                if char in cdict:\n",
    "                    sample[count] = cdict[char]\n",
    "                    count += 1\n",
    "                if count >= FEATURE_LEN-1:\n",
    "                    break\n",
    "        samples[label].append(sample)\n",
    "    samples_per_class = None\n",
    "    classes = samples.keys()\n",
    "    class_samples = []\n",
    "    for c in classes:\n",
    "        if samples_per_class is None:\n",
    "            samples_per_class = len(samples[c])\n",
    "        else:\n",
    "            assert samples_per_class == len(samples[c])\n",
    "        class_samples.append(samples[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_onehot(input_):\n",
    "    target = np.zeros(len(classes))\n",
    "    target[input_] = 1\n",
    "    return target\n",
    "y= []\n",
    "for i in range(len(classes)):\n",
    "    for j in range(samples_per_class):\n",
    "        target =build_onehot(i)\n",
    "        y.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.reshape(class_samples,(-1,FEATURE_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.reshape(y,(-1,len(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "# Split train/test set\n",
    "n_dev_samples = 10000\n",
    "# TODO: Create a fuckin' correct cross validation procedure\n",
    "x_train, x_dev = x_shuffled[:-n_dev_samples], x_shuffled[-n_dev_samples:]\n",
    "y_train, y_dev = y_shuffled[:-n_dev_samples], y_shuffled[-n_dev_samples:]\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from inception.slim import ops\n",
    "import tflearn\n",
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    based on the Very Deep Convolutional Networks for Natural Language Processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=14, filter_size=3,\n",
    "                 l2_reg_lambda=0.001, sequence_max_length=1014, num_quantized_chars=71,embedding_size=16):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None,sequence_max_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.training =  tf.placeholder(tf.int32, name=\"trainable\")\n",
    "        if self.training==1:\n",
    "            TRAIN = True\n",
    "        else:\n",
    "            TRAIN = False\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        # ================ embedding character ================\n",
    "        with tf.device('/cpu:0'),tf.name_scope(\"embedding\"):\n",
    "            W0 = tf.Variable(tf.random_uniform([num_quantized_chars, embedding_size], -1.0, 1.0),name=\"W\")\n",
    "            self.embedded_characters = tf.nn.embedding_lookup(W0,self.input_x)\n",
    "            self.embedded_characters_expanded = tf.expand_dims(self.embedded_characters,-1,name=\"embedding_input\")\n",
    "        # ================ Layer 1 ================\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            num_filters_layer_1 = 64\n",
    "            filter_shape = [3, embedding_size, 1, num_filters_layer_1]\n",
    "            filter1 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"filter1\")\n",
    "            self.conv1 = tf.nn.conv2d(self.embedded_characters_expanded, filter1, strides=[1, 1,embedding_size, 1], padding=\"SAME\",\n",
    "                                     name=\"conv1\")\n",
    "            self.batch_normal0=tflearn.layers.normalization.batch_normalization(self.conv1,trainable= TRAIN,\n",
    "                                                                                name=\"BatchNormalization0\")\n",
    "           \n",
    "        # ================ Layer 2 ================\n",
    "        with tf.name_scope(\"conv-maxpool-2_3\"):\n",
    "            filter_shape1 = [3, 1, 64, 64]\n",
    "            filter_shape2 = [3, 1, 64, 64]\n",
    "            filter_1 = tf.Variable(tf.truncated_normal(filter_shape1, stddev=0.05), name=\"W1\")\n",
    "            filter_2 = tf.Variable(tf.truncated_normal(filter_shape2, stddev=0.05), name=\"W2\")\n",
    "            conv1 = tf.nn.conv2d(self.batch_normal0, filter_1, strides=[1, 1, filter_shape1[1], 1], padding=\"SAME\")\n",
    "            batch_normal1 =tflearn.layers.normalization.batch_normalization(conv1,trainable= TRAIN,name=\"BatchNormalization1\")\n",
    "            h1 = tf.nn.relu(batch_normal1)\n",
    "            self.conv2 = tf.nn.conv2d(h1, filter_2, strides=[1, 1, filter_shape2[1], 1], padding=\"SAME\")\n",
    "            batch_normal2 =tflearn.layers.normalization.batch_normalization(self.conv2,trainable=TRAIN,name=\"BatchNormalization2\")\n",
    "            h2 = tf.nn.relu(batch_normal2)\n",
    "            self.pooled1 = tf.nn.max_pool(h2,ksize=[1, 3, 1, 1],strides=[1, 2, 1, 1],padding='SAME',name=\"pool1\")\n",
    "            \n",
    "        # ================ Layer 3 ================\n",
    "        with tf.name_scope(\"conv-maxpool-6_7_8_9\"):\n",
    "            filter_shape3 = [3, 1, 64,128]\n",
    "            filter_shape4 = [3, 1, 128, 128]\n",
    "            filter_3 = tf.Variable(tf.truncated_normal(filter_shape3, stddev=0.05), name=\"W3\")\n",
    "            filter_4 = tf.Variable(tf.truncated_normal(filter_shape4, stddev=0.05), name=\"W4\")\n",
    "            conv4 = tf.nn.conv2d(self.pooled1, filter_3, strides=[1, 1, filter_shape3[1], 1], padding=\"SAME\")\n",
    "            batch_normal3 =tflearn.layers.normalization.batch_normalization(conv4,trainable= TRAIN,name=\"BatchNormalization3\")\n",
    "            h3= tf.nn.relu(batch_normal3)\n",
    "            conv5 = tf.nn.conv2d(h3, filter_4, strides=[1, 1, filter_shape4[1], 1], padding=\"SAME\")\n",
    "            batch_normal4 =tflearn.layers.normalization.batch_normalization(conv5,trainable= TRAIN,name=\"BatchNormalization4\")\n",
    "            h4 = tf.nn.relu(batch_normal4)\n",
    "            self.pooled2 = tf.nn.max_pool(h4,ksize=[1, 3, 1, 1],strides=[1, 2, 1, 1],padding='SAME',name=\"pool1\")\n",
    "        # ================ Layer 4 ================\n",
    "        with tf.name_scope(\"conv-maxpool-10_11_12_13\"):\n",
    "            filter_shape5 = [3, 1, 128,256]\n",
    "            filter_shape6 = [3, 1, 256, 256]\n",
    "            filter_5 = tf.Variable(tf.truncated_normal(filter_shape5, stddev=0.05), name=\"W3\")\n",
    "            filter_6 = tf.Variable(tf.truncated_normal(filter_shape6, stddev=0.05), name=\"W4\")\n",
    "            conv6 = tf.nn.conv2d(self.pooled2, filter_5, strides=[1, 1, filter_shape5[1], 1], padding=\"SAME\")\n",
    "            batch_normal5 =tflearn.layers.normalization.batch_normalization(conv6,trainable= TRAIN,name=\"BatchNormalization5\")\n",
    "            h5= tf.nn.relu(batch_normal5)\n",
    "            conv7 = tf.nn.conv2d(h5, filter_6, strides=[1, 1, filter_shape6[1], 1], padding=\"SAME\")\n",
    "            batch_normal6 =tflearn.layers.normalization.batch_normalization(conv7,trainable= TRAIN,name=\"BatchNormalization6\")\n",
    "            h6= tf.nn.relu(batch_normal6)\n",
    "            self.pooled3 = tf.nn.max_pool(h6,ksize=[1, 3, 1, 1],strides=[1, 2, 1, 1],padding='SAME',name=\"pool1\")\n",
    "        # ================ Layer 5 ================\n",
    "        with tf.name_scope(\"conv-maxpool-14_15_16_17\"):\n",
    "            filter_shape7 = [3, 1, 256,512]\n",
    "            filter_shape8 = [3, 1, 512, 512]\n",
    "            filter_7 = tf.Variable(tf.truncated_normal(filter_shape7, stddev=0.05), name=\"W3\")\n",
    "            filter_8 = tf.Variable(tf.truncated_normal(filter_shape8, stddev=0.05), name=\"W4\")\n",
    "            conv8 = tf.nn.conv2d(self.pooled3, filter_7, strides=[1, 1, filter_shape7[1], 1], padding=\"SAME\")\n",
    "            batch_normal7 =tflearn.layers.normalization.batch_normalization(conv8,trainable= TRAIN,name=\"BatchNormalization7\")\n",
    "            h7= tf.nn.relu(batch_normal7)\n",
    "            conv9 = tf.nn.conv2d(h7, filter_8, strides=[1, 1, filter_shape8[1], 1], padding=\"SAME\")\n",
    "            batch_normal8 =tflearn.layers.normalization.batch_normalization(conv9,trainable= TRAIN,name=\"BatchNormalization8\")\n",
    "            h8= tf.nn.relu(batch_normal8)\n",
    "            self.pooled4 = tf.nn.max_pool(h8,ksize=[1, 13, 1, 1],strides=[1, 13, 1, 1],padding='VALID',name=\"pool3\")\n",
    "            self.after_k_maxpool = tf.reshape(self.pooled4,(-1,512*9))\n",
    "        # ================ fc-3 ================\n",
    "        # I just use 2 layers\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            W1 = tf.Variable(tf.truncated_normal([4608, 2048], stddev=0.05), name=\"W1\")\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[2048]), name=\"b1\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(self.after_k_maxpool, W1, b1), name=\"fc-1-out\")\n",
    "        '''\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W2 = tf.Variable(tf.truncated_normal([2048, 2048], stddev=0.05), name=\"W2\")\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[2048]), name=\"b2\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "            self.fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(self.fc_1_output, W2, b2), name=\"fc-2-out\")\n",
    "        '''\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W3 = tf.Variable(tf.truncated_normal([2048, num_classes], stddev=0.05), name=\"W3\")\n",
    "            b3= tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b3\")\n",
    "            l2_loss += tf.nn.l2_loss(W3)\n",
    "            l2_loss += tf.nn.l2_loss(b3)\n",
    "            self.scores = tf.nn.relu(tf.nn.xw_plus_b(self.fc_1_output, W3, b3), name=\"fc-3-out\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "            # ================ Loss and Accuracy ================\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "def batch_iter(x, y, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    # data = np.array(data)\n",
    "    data_size = len(x)\n",
    "    num_batches_per_epoch = int(data_size/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"In epoch >> \" + str(epoch + 1))\n",
    "        print(\"num batches per epoch is: \" + str(num_batches_per_epoch))\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            x_shuffled = x[shuffle_indices]\n",
    "            y_shuffled = y[shuffle_indices]\n",
    "        else:\n",
    "            x_shuffled = x\n",
    "            y_shuffled = y\n",
    "        for batch_num in range(num_batches_per_epoch-1):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            x_batch = x_shuffled[start_index:end_index]\n",
    "            y_batch = y_shuffled[start_index:end_index]\n",
    "            batch = list(zip(x_batch, y_batch))\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size =128\n",
    "num_epochs = 20\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = CharCNN()\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\n",
    "                    \"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\n",
    "                    \"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(\n",
    "            os.path.curdir, \"runs_new\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary(\n",
    "            [loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(\n",
    "            train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already\n",
    "        # exists, so we need to create it.\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.training:1,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }       \n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op,\n",
    "                    cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            # write fewer training summaries, to keep events file from\n",
    "            # growing so big.\n",
    "            if step % (evaluate_every / 2) == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(\n",
    "                    time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            dev_size = len(x_batch)\n",
    "            max_batch_size = 500\n",
    "            num_batches = dev_size/max_batch_size\n",
    "            acc = []\n",
    "            losses = []\n",
    "            print(\"Number of batches in dev set is \" + str(num_batches))\n",
    "            for i in range(num_batches):\n",
    "                x_batch_dev = x_batch[i * max_batch_size:(i + 1) * max_batch_size]\n",
    "                y_batch_dev = y_batch[i * max_batch_size: (i + 1) * max_batch_size]\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch_dev,\n",
    "                  cnn.input_y: y_batch_dev,\n",
    "                  cnn.training:0,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                acc.append(accuracy)\n",
    "                losses.append(loss)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"batch \" + str(i + 1) + \" in dev >>\" +\" {}: loss {:g}, acc {:g}\".format(time_str, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "            print(\"\\nMean accuracy=\" + str(sum(acc)/len(acc)))\n",
    "            print(\"Mean loss=\" + str(sum(losses)/len(losses)))\n",
    "        # just for epoch counting\n",
    "        num_batches_per_epoch = int(len(x_train)/batch_size) + 1\n",
    "        # Generate batches\n",
    "        batches = batch_iter(x_train, y_train,batch_size, num_epochs)\n",
    "        evaluate_every = 300\n",
    "        checkpoint_every =300\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"Epoch: {}\".format(\n",
    "                    int(current_step / num_batches_per_epoch)))\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(\n",
    "                    sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
